https://staff.fnwi.uva.nl/p.j.c.spreij/onderwijs/master/mtp.pdf
https://web.math.princeton.edu/~js129/PDFs/teaching/MAT425_spring_2025/MAT425_Lecture_Notes.pdf
TODO: take a refresher on transfinite induction https://de.wikipedia.org/wiki/Transfinite_Induktion
# 1. $\sigma$-Algebras and measures

For any non-empty set $S$ we can select some collection of subsets $\Sigma_0 \subset 2^S$.
Such a collection of subsets is called an *algebra* on S if:
	(i) $S \in \Sigma_0$,
	(ii) $E \in \Sigma_0 \Rightarrow E^c \in \Sigma_0$,
	(iii) $E, F \in \Sigma_0 \Rightarrow E \cup F \in \Sigma_0$.
We notice immediately that the empty set $\emptyset$ always belongs to any algebra, since algebras contain the original set $S$ by (i) and are closed under complements (ii), thus the empty set, as the complement of the original set, must be member of any algebra. 

Property (iii) extends to finite unions by induction.

Furthermore, we can show that property (iii) also implies closure under finite intersections and finite set differences, as can be shown:
	$E \cap F=\left(E^c \cup F^c\right)^c$ 
	Leading to:
	$E, F \in \Sigma_0 \Rightarrow E \cap F \in \Sigma_0$,
	and analogously:
	$E \backslash F=E \cap F^c \in \Sigma_0$

### 1.2 Definition of $\sigma$-Algebra

For any nonempty set $S$, a collection of subsets $\Sigma \subset 2^S$ is called a $\sigma$-Algebra on S if it is an algebra and closed under countable unions: 
	$\bigcup_{n=1}^{\infty} E_n \in \Sigma$ for any $E_n \in \Sigma$ $(n = 1, 2, \ldots)$.

A pair $(S, \Sigma)$ is called a measurable space. Elements of $\Sigma$ are called measurable sets. 

#### 1.2.1 $\sigma$-Operator
For any collection $\mathcal{C}$ of subsets of a set $S$ we define the $\sigma$-Operator $\sigma(\mathcal{C})$. This Operator yields the smallest $\sigma$-Algebra that contains $\mathcal{C}$. $\sigma(\mathcal{C})$ is therefore the intersection of all $\sigma$-algebras that contain $\mathcal{C}$. This is very similar to the way we define inner and outer measures in Real Analysis, where we defined the smallest open cover of an interval as the intersection of all open covers of the respective interval. 
We say that $\mathcal{C}$ generates $\Sigma$. 
The union of two $\sigma$-algebras is not necessarily a $\sigma$-algebra. We write
$\Sigma_1 \vee \Sigma_2$ for $\sigma\left(\Sigma_1 \cup \Sigma_2\right)$ instead.
Note: On countable sets we can often just use the power set as a $\sigma$-algebra if we don't explicitly need the smallest $\sigma$-Algebra generated by the countable set.

#### 1.2.2 Borel sets and the Borel $\sigma$-algebra
Let $\mathcal{O}$ be the collection of all open subsets of $\mathbb{R}$ with the usual topology (in which all intervals (a, b) are open). Then we define
$\mathcal{B}:=\sigma(\mathcal{O})$.
As the Borel $\sigma$-Algebra. This notion can get quite abstract for sets in the general sense, but for the real numbers we can construct $\mathcal{B}$ like so: 

### Proposition 1.3 Construction of $\mathcal{B}$

Let $\mathcal{I}=\{(-\infty, x]: x \in \mathbb{R}\}$. Then $\sigma(\mathcal{I})=\mathcal{B}$.
**Proof:** First we show that any half open interval $(-\infty, x]$ can be expressed as a countable union of open intervals: 
	$(-\infty, x]=\cap_n\left(-\infty, x+\frac{1}{n}\right)$ 
Because $\sigma$-Algebras are closed under countable unions and thus, by De Morgan, closed under countable intersections we have shown that $\mathcal{I} \subset \mathcal{B}$ and $\sigma(\mathcal{I}) \subset \mathcal{B}$ since $\sigma(\mathcal{I})$ is the smallest $\sigma$-Algebra that contains $\mathcal{I}$. 
For the reverse inclusion we first observe that any open interval, by the definition of openness, can be expressed as a countable union of half open intervals like so: 
	$(-\infty, x)=\cup_n\left(-\infty, x-\frac{1}{n}\right] \in \sigma(\mathcal{I})$.
Thus, we can express any open interval $(a, b)$ as:
	$(a, b)=(-\infty, b) \backslash(-\infty, a] \in \sigma(\mathcal{I})$.
Where the inclusion in $\sigma(\mathcal{I})$ holds since algebras are closed under set difference.
We can now proceed by using the density, and countability, of the Rational numbers as follows: 
Let $G$ be an arbitrary open set.
By openness there must exist a rational number $\varepsilon_x>0$ for every $x \in G$ such that the open interval $\left(x-2 \varepsilon_x, x+2 \varepsilon_x\right) \subset G$. 
Now, considering the open interval $\left(x-\varepsilon_x, x+\varepsilon_x\right)$ nested in our original interval, we can pick, by density of the Rationals, any rational $q_x$ in this interval. For any such $q_x$ it holds that: $\left|x-q_x\right| \leq \varepsilon_x$.
We can see that: 
	$x \in\left(q_x-\varepsilon_x, q_x+\varepsilon_x\right) \subset \left(x-2 \varepsilon_x, x+2 \varepsilon_x\right) \subset G$
Meaning, that we can express our original $G$ as a countable union of such open intervals: 
	$G \subset \cup_{x \in G}\left(q_x-\varepsilon_x, q_x+\varepsilon_x\right) \subset G$
Where the union is indeed countable, because there exist only countably many rational points $q_x$ and $\varepsilon_x$.
This means in turn, that $G$ is indeed part of the $\sigma$-Algebra of open intervals: 
	$G \in \sigma(\mathcal{I})$  
Which leads us to the desired conclusion:
	$\mathcal{O} \subset \sigma(\mathcal{I})$ and therefore
	$\mathcal{B} \subset \sigma(\mathcal{I})$. (Since $\mathcal{B}$) was defined as the smallest such $\sigma$-Algebra. $\square$

Does any subset of $\mathbb{R}$ fall into $\mathcal{B}$? 
No. Vitali sets are a popular counter example, usually employed in the context of the measure problem, to show that there cannot exist a general measure function that assigns any subset of $\mathbb{R}$ a valid measure. One can show that the cardinality of $\mathcal{B}(\mathbb{R})$ is the same as the cardinality of $\mathbb{R}$ itself, leading to the same conclusion. I skip this proof here, please refer to the linked text for the full proof. The general idea is to show that both the cardinality of $\mathcal{B}(\mathbb{R})$ and the cardinality of the reals are equivalent (intuitively because the reals have the same cardinality as the power set of the natural numbers/the set of all infinite binary sequences, and we generate all substs of $\mathcal{B}(\mathbb{R})$ with a countable number of operations). Thus, when one shows that both of these sets have the same cardinality we notice that the powerset of the reals is of strictly larger cardinality $2^{\mathfrak{c}}$ which implies that there must be some subsets of the reals that are not contained in $\mathcal{B}(\mathbb{R})$.

### 1.2 Measures

In the context of probability we're interested in set functions that assign any subset of a sigma algebra a number ("measurement"). This function should also adhere to certain restrictions.

Let $S$ be any set and $\Sigma$ any $\sigma$-Algebra on $S$. A mapping $\mu: \Sigma \rightarrow[0, \infty]$ is 
	(i) *finitely additive* if $\mu(\emptyset)=0$ and $\mu(E \cup F)=\mu(E)+\mu(F)$ for every pair of disjoint sets $E$ and $F$.
	(ii) *$\sigma$-additive* if $\mu(\emptyset)=0$ and if $\mu\left(\cup_n E_n\right)=\sum_n \mu\left(E_n\right)$ for any sequence of disjoint sets of $\Sigma$ for which its union is also in $\Sigma$. (Which is technically fulfilled by additivity and closure under countable unions already).

#### Definition: Measure Space
Let the tuple $(S, \Sigma)$ be a measurable space. Endowed with a countably additive mapping $\mu: \Sigma \rightarrow[0, \infty]$ we obtain the triple $(S, \Sigma, \mu)$ $-$ a *measure space*.

Measures can have certain useful properties: 
	1. If $\mu(S)<\infty$ we say $\mu$ is finite
	2. If $\mu\left(S_n\right)<\infty$ for $S_n$ measurable sets and $S=\cup_n S_n$ we say $\mu$ is $\sigma$-finite
	3. If $\mu(S)=1$ we say $\mu$ is a *probability measure*

Simple examples for measures: 
* The *counting measure* $\tau$ on $2^{\mathbb{N}}$: $\tau(E)=|E|$ for any $E \in 2^{\mathbb{N}}$ 
* The *Dirac measure* $\delta(E)=\mathbf{1}_E\left(x_0\right)$, for $E \in \Sigma$
* The *Lebesgue measure* on the Borel $\sigma$-Algebra of the reals. 

#### Theorem 1.5: Existence and uniqueness of Lebesgue Measure
There exists a unique measure $\lambda$ on $(\mathbb{R}, \mathcal{B})$ with the property that for any interval $I=(a, b]$ with $a<b$ it holds that $\lambda(I)=b-a$.
**Proof:**
Deferred to later

We will make use of the Lebesgue measure in the Caratheodory Extension Theorem where we will show that $\mathcal{B}$ is no longer the largest $\sigma$-Algebra for which $\lambda$ can be coherently defined. We will extend $\lambda$ to a larger such $\sigma$-Algebra and then prove that by restricting $\lambda$ to our strictly smaller target $\sigma$-Algebra our initial assumptions about $\lambda$ still hold. More on that later.

#### Proposition 1.6: Rules for Measure Calculations
Let $(S, \Sigma, \mu)$ be a measure space. Then for all Sets $E, F \in \Sigma$ the following holds:
	(i) If $E \subset F$, then $\mu(E) \leq \mu(F)$.
	(ii) $\mu(E \cup F) \leq \mu(E)+\mu(F)$.
	(iii) $\mu\left(\cup_{k=1}^n E_k\right) \leq \sum_{k=1}^n \mu\left(E_k\right)$
	If $\mu$ is finite, we also have
	(iv) If $E \subset F$, then $\mu(F \backslash E)=\mu(F)-\mu(E)$.
	(v) $\mu(E \cup F)=\mu(E)+\mu(F)-\mu(E \cap F)$.
**Proof**: In (i) we can split the set $F$ into a disjoint union $F=E \cup(F \backslash E)$. Using the non-negative property of measures we can immediately see that the inequality holds. If $\mu$ is finite property (iv) also directly follows. To prove (ii) we first show that $E\cup F$ can be written as a disjoint Union $E \cup(F \backslash(E \cap F))$ where $E \cap F \subset F$. Applying (i) we directly obtain (ii) and via induction we also obtain (iii). (v) follows, analogous to these considerations, from application of (iv). $\square$

#### Proposition 1.7: Continuity Properties for Measures
Let $(E_n)$ be a sequence of sets in $\Sigma$.
	(i) If the sequence is increasing, with limit $E=\cup_n E_n$, then $\mu\left(E_n\right) \uparrow \mu(E)$ as $n \rightarrow \infty$ 
	(ii) If the sequence is decreasing, with limit $E=\cap_n E_n$ and if $\mu\left(E_n\right)<\infty$ from a certain index on, then $\mu\left(E_n\right) \downarrow \mu(E)$ as $n \rightarrow \infty$.
**Proof:** To prove (i) we define helper sets $D_i$ that are disjoint and thus help us "build up" all sequences of sets $(E_n)$ as disjoint unions of them. We then use [[Measure Theoretic Probability#Proposition 1.6 Rules for Measure Calculations]] to show the statement holds.
To prove (ii) we assume w.l.o.g. that $\mu\left(E_1\right)<\infty$. Again, we define a helper set $F_n$ as $F_n=E_1 \backslash E_n$ and note that $(F_n)$ is an increasing sequence with limit which allows us to use (i) directly to obtain (ii). $\square$

#### Corrolary 1.8: $\sigma$-sub-Additivity
Let $(S, \Sigma, \mu)$ be a measure space. For an arbitrary sequence $(E_n)$ of sets in $\Sigma$, we have $\mu\left(\cup_{n=1}^{\infty} E_n\right) \leq \sum_{n=1}^{\infty} \mu\left(E_n\right)$.
Proof: [[Measure Theoretic Probability#1.6 Exercises]] Exercise 2

### 1.3 Null Sets and Completeness of Measure Spaces

Consider a measure space $(S, \Sigma, \mu)$ and let any $E \in \Sigma$ be such that $\mu(E)=0$. If $N$ is a subset of $E$, then it is fair to suppose that also $\mu(N)=0$. But this only holds if $N \in \Sigma$. 

Intuition: $\Sigma$ may not be "closed" under the measure operation, meaning there might exist some subsets of sets with measure zero that are not measurable under $\mu$.

New terminology: A set $N \subset S$ is called a *null set* or $\mu$-null set, if there exists $E \in \Sigma$ with $E \supset N$ and $\mu(E)=0$. 
The collection of null sets is denoted by $\mathcal{N}$, or $\mathcal{N}_\mu$ since it depends on the choice of $\mu$.
We will show later that $\mathcal{N}$ is a $\sigma$-Algebra and that we can extend $\mu$ to $\bar{\Sigma}=\Sigma \vee \mathcal{N}$.
By extending $\mu$ to $\bar{\Sigma}$ we obtain an extended measure space $(S, \bar{\Sigma}, \bar{\mu})$, which is complete under the measure $\bar{\mu}$ since all $\bar{\mu}$-null sets belong to the $\sigma$-algebra $\bar{\Sigma}$.

### 1.4 $\pi$- and $d$-systems

The contents of the systems of sets we encounter in probability theory and functional analysis are often quite abstract and it can be hard to make constructive statements about the structure of the sets they contain. We will see how some "well behaved" systems of sets allow us to deduce properties of $\Sigma$ from the properties of $\mathcal{C}$ for $\sigma(\mathcal{C})=\Sigma$.

#### Definition 1.10: $\pi$-system

A collection $\mathcal{I}$ of subsets of $S$ is called a $\pi$-system if it is closed under pairwise intersection: 
	$I_1, I_2 \in \mathcal{I}$ implies $I_1 \cap I_2 \in \mathcal{I}$.

Corollary: By induction we can show that this also implies that $pi$-systems are closed under finite intersections.

How is this useful for us, given our stronger definition of $\sigma$-Algebra?
$\sigma$-Algebras allow *countably* many set operations. 
It is possible to disentangle the defining properties of a $\sigma$-Algebra into:
	(i) taking finite intersections
	(ii) the defining properties of a $d$-system / $Dynkin$-system

#### Definition 1.11: $d$-system / $Dynkin$-system

A collection $\mathcal{D}$ of subsets of $S$ is called a $d$-system if the following conditions hold:
	(i) $S \in \mathcal{D}$.
	(ii) If $E, F \in \mathcal{D}$ such that $E \subset F$, then $F \backslash E \in \mathcal{D}$. (note this is a less general assumption than general set differences)
	(iii) If $E_n \in \mathcal{D}$ for $n \in \mathbb{N}$, and $E_n \subset E_{n+1}$ for all $n$, then $\cup_n E_n \in \mathcal{D}$.

#### Proposition 1.12: $\Sigma$ is a $\sigma$-algebra iff it is a $\pi$-system and a $d$-system.

**Proof** Outline: We need to get from the combined properties of pi and dynkin systems to the properties of sigma-Algebras. We already get finite intersections for free. Can we somehow use the "nested difference" property of Dynkin systems with the countable union property to arrive at a full $\sigma$-Algebra? Implication from $\sigma$-Algebra to pi and d-systems is trivial.
First we observe that the "Universe" property is given in d-systems. We use this in combination with (ii) to show that since every subset of a d-system is at least subset of the "universe" set we recover closure under the complement operation. Now we can derive that our system is thus closed under finite unions sine we can express a union as a complement of an intersection of complements, and $\Sigma$ is closed under intersections because it is a $pi$-system. We finally combine this with the third property of dynkin systems to recover that $\Sigma$ must be a $\sigma$-Algebra.
Clarification: We show the countable union property by first defining auxiliary sets $B_n$ like so: 
	$B_1=A_1, \quad B_n=A_n \backslash \bigcup_{k=1}^{n-1} A_k$ for $n \geq 2$.
Where it is clear that each $B_n \subseteq A_n$, the $B_n$ are pairwise disjoint, and $\bigcup_{n=1}^{\infty} A_n=\bigcup_{n=1}^{\infty} B_n$. We only utilize finite intersections, difference and disjoint countable unions, which we all have shown to be valid operations for $\Sigma$. So we can conclude that $B_n \in \Sigma$ for all $n_r$ and finally:
	$\bigcup_{n=1}^{\infty} A_n=\bigcup_{n=1}^{\infty} B_n \in \Sigma$ $\qquad \square$

This leads us straight to Dynkin's Lemma:
#### Proposition 1.13: Dynkin's Lemma: 
Let $\mathcal{I}$ be a $\pi$-system. Then $d(\mathcal{I})=\sigma(\mathcal{I})$.
Proof in the linked notes.
Proposition 1.13 is equivalent to: 
Let $\mathcal{I}$ be a $\pi$-system and $\mathcal{D}$ be a $d$-system. If $\mathcal{I} \subset \mathcal{D}$, then $\sigma(\mathcal{I}) \subset \mathcal{D}$.

Which allows us to show that any finite measure on $\Sigma$ is characterized by its action on a rich enough $\pi$-system. (If we have two **finite** measures $\mu$ and $\nu$ defined on a $\sigma$-Algebra $\Sigma$, and they agree on a $\pi$-system $\mathcal{P} \subseteq \Sigma$ that generates $\Sigma$, then they must coincide on all of $\Sigma$).

#### Theorem 1.15: Uniqueness of Measures

Let $\mathcal{I}$ be a $\pi$-system and $\Sigma=\sigma(\mathcal{I})$. Let $\mu_1$ and $\mu_2$ be finite measures on $\Sigma$ with the properties that $\mu_1(S)=\mu_2(S)$ and that $\mu_1$ and $\mu_2$ coincide on $\mathcal{I}$.  Then $\mu_1=\mu_2$ (on $\Sigma)$. (The measures coincide on the entire sigma Algebra.)
Note: Without finiteness for the measures this theorem does not hold.

### 1.5: Probability Language

In Probability Theory, we are used to the notation $(\Omega, \mathcal{F}, \mathbb{P})$ where we define such a specific instance of a measure space, equipped with a *probability* measure as a *probability space*.
In this context, the measure $\mathbb{P}$ is normed to $\mathbb{P}(\Omega)=1$.  where we refer to $\Omega$ as set of outcomes with the respective *events* being subsets of $\mathcal{F}$. 
**An event is a measurable subset of the set of all outcomes.**

A probability space can be interpreted as a mathematical model of a random experiment. 

In many interesting cases one needs to be careful how one specifies a probability measure to a probability space. Consider infinite sequences of coin tosses. The set of outcomes would take the form $\Omega=\{0,1\}^{\mathbb{N}}$ with each potential outcome being in infinite sequence $\left(\omega_1, \omega_2, \ldots\right)$ with $\omega_n \in\{0,1\}$. Can we just use the powerset of $\Omega$ as a suitable $\sigma$-Algebra?
No! We can show that there is a bijection between the set of all infinite binary sequences (our set of events) and any interval on the real number line, meaning that we run into the same cardinality problem we already encountered when trying to show that there are subsets of $\mathbb{R}$ that are not part of $\mathfrak{B}(\mathbb{R})$. 
How can we find a suitable candidate then?
An alternative would be to define $\mathcal{C}$ as the collection of all events:
	$\mathcal{C}=\left\{\left\{\omega \in \Omega: \omega_n=s\right\}, n \in \mathbb{N}, s \in\{0,1\}\right\}$
Where the n-th entry corresponds to some outcome. 
One can then show that there exists a probability measure on $\mathcal{F}=\sigma(\mathcal{C})$ that recovers all finite cases nicely. Proof: TODO

#### Limsup and Liminf

Interpreting $\mathcal{F}$ as a collection of events one can introduce limsup and liminf as special events.
Consider a sequence of events $E_1, E_2, \dots$ and define:
	$\begin{aligned} \limsup E_n & :=\bigcap_{m=1}^{\infty} \bigcup_{n=m}^{\infty} E_n \\ \liminf E_n & :=\bigcup_{m=1}^{\infty} \bigcap_{n=m}^{\infty} E_n .\end{aligned}$
Limsup is described as the event that the $E_n$ occur infinitely often.
Liminf is the event that the $E_n$ occur eventually.
Proofs/Intuition in exercises. 




### 1.6 Exercises

#### 1.6.2 Prove [[Measure Theoretic Probability#Corrolary 1.8 $ sigma$-sub-Additivity]]







# 2 Existence of Lebesgue Measure

To construct the Lebesgue Measure on the Borel sets of $\mathbb{R}$ we begin by defining a suitable countably additive function on an algebra, extend it to a $\sigma$-Algebra strictly larger than $\mathfrak{B}(\mathbb{R})$ and then show that all desirable properties of our target function still hold when we restrict ourselves back to the Borel-$\sigma$-Algebra of the reals. 
This is the core idea of the *Carathéodory Extension Theorem*.

### 2.1 Outer Measure & Construction

We define an *outer measure* on a set $S$ as a mapping $\mu^*: 2^S \rightarrow [0, \infty]$ that satisfies: 
	(i) $\mu^*(\emptyset)=0$,
	(ii) $\mu^*$ is monotone, i.e. $\mu^*(E) \leq \mu^*(F)$ if $E \subset F$,
	(iii) $\mu^*$ is subadditive, i.e. $\mu^*\left(\bigcup_{n=1}^{\infty} E_n\right) \leq \sum_{n=1}^{\infty} \mu^*\left(E_n\right)$, valid for any sequence of sets $E_n$

We immediately notice that $\mu^*$ is defined on the powerset of $S$ which means that it is defined even for sets that aren't measurable according to our prior definition. Can we somehow restrict this to a valid measure on the Borel sets for example?
Additional note: This is also a slightly different approach to the way Kolmogorov introduces the concept of a general outer measure in chapter 25 section 2 of "Introductory Real Analysis", where he introduces outer measures as the greatest lower bound over all coverings of $E$ by a finite or countable system of rectangles $P_k$. We arrive at measurability by showing that the inner and outer measure of sets are equal to the measure itself, which is conceptually simpler but doesn't allow us to show that we can extend set functions defined on simple subsets to measures on $\sigma$-Algebras.
This is why we take a slightly different approach, which is a bit more abstract.
### 2.2 Definition $\mu$-measurable
Let $\mu^*$ be an outer measure on a set $S$. A subset $E \subset S$ is called $\mu$-measurable if 
	$\mu^*(F)=\mu^*(E \cap F)+\mu^*\left(E^c \cap F\right),\quad \forall F \subset S$ 
This means that a $\mu$-measurable subset "splits" each other subset of $S$ additively. (This will come in handy for the construction of the Carathéodory $\sigma$-Algebra later.)
The class of all such $\mu$-measurable sets, also known as the Carathéodory sets, is denoted by $\Sigma_\mu$.
### 2.3 Carathéodory Criterion
Let $\mu^*$ be an outer measure on a set $S$. Then $\Sigma_\mu$ is a $\sigma$-Algebra and the restricted mapping $\mu: \Sigma_\mu \rightarrow[0, \infty]$ of $\mu^*$ is a measure on $\Sigma_\mu$.

**Proof**: We immediately see that the empty set $\emptyset$ is part of $\Sigma_\mu$ as any intersection with the empty set remains empty, which guarantees that $\emptyset$ is $\mu$-measurable.
Further, $E^c \in \Sigma_\mu$ as soon as $E \in \Sigma_\mu$, meaning that $\Sigma_\mu$ is closed under complements.

Let $E_1, E_2 \in \Sigma_\mu$ and $F \subset S$. The Identity: 
	$F \cap\left(E_1 \cap E_2\right)^c=\left(F \cap E_1^c\right) \cup\left(F \cap\left(E_1 \cap E_2^c\right)\right)$
yields, using the subadditivity of $\mu^*$, 
	$\mu^*\left(F \cap\left(E_1 \cap E_2\right)^c\right) \leq \mu^*\left(F \cap E_1^c\right)+\mu^*\left(F \cap\left(E_1 \cap E_2^c\right)\right)$.
Now, after adding $\mu^*\left(F \cap\left(E_1 \cap E_2\right)\right)$ to both sides and using that $E_1, E_2 \in \Sigma_\mu$ we obtain: 
	$\mu^*\left(F \cap\left(E_1 \cap E_2\right)\right)+\mu^*\left(F \cap\left(E_1 \cap E_2\right)^c\right) \leq \mu^*(F)$,
where we can show equality by using subadditivity of $\mu$ one more time. We have thus shown that $\Sigma_\mu$ is closed under intersections, and is thus an algebra. 

We can then show, by induction and using the monotonicity property of $\mu^*$, that for any pairwise disjoint sequence of sets $E_i$ the following equality holds: 
	$\mu^*(F \cap E)=\sum_{i=1}^{\infty} \mu^*\left(F \cap E_i\right)$

Combining this one last time with monotonicity of $\mu$ for any countable union of sets $U_n=\bigcup_{i=1}^n E_i$ such that $U_n \in \Sigma_\mu$. we can finally show that $\Sigma_\mu$ is closed under countable unions and thus it is a $\sigma$-Algebra. $\square$

We now use Theorem 2.3 to show the existence of Lebesgue measure on $(\mathbb{R}, \mathcal{B})$.
(This is analogous to how Kolmogorov introduces the idea of outer measure in "Introductory Real Analysis). 
Let $E$ be a subset of $\mathbb{R}$. By $\mathcal{I}(E)$ we denote a cover of $E$ consisting of at most countably many open intervals. For any interval $I$, we denote by $\lambda_0(I)$ its ordinary length. We now define a function $\lambda^*$ on $2^\mathbb{R}$ like so:
	$\lambda^*(E)=\inf _{\mathcal{I}(E)} \sum_{I_k \in \mathcal{I}(E)} \lambda_0\left(I_k\right)$.
In Lemma 2.4 of MTP we proceed to show that this is indeed a valid outer measure. The empty set criterion and monotonicity are immediately obvious / or can at least be proven easily by observing the infimum property of this definition. Kolmogorov just proceeds with the analogous definition of inner measure but for full rigor we would need to prove subadditivity.

### Lemma 2.5: 
Any Interval $I_a=(-\infty, a] \quad(a \in \mathbb{R})$ is $\lambda$-measurable. $I_a \in \Sigma_\lambda$. And thus $\mathcal{B} \subset \Sigma_\lambda$.

**Proof**: Let $E \subset \mathbb{R}$. We already know that $\lambda^*$ is subadditive, so we only need to show that for any interval the "additive decomposable"/ Carathéodory Criterion holds: 
	$\lambda^*\left(E \cap I_a\right)+\lambda^*\left(E \cap I_a^c\right)$.
To accomplish this we choose a cover $\mathcal{I}(E)$ such that $\lambda^*(E) \geq\sum_{I \in \mathcal{I}(E)} \lambda^*(I)-\varepsilon$, for any $\varepsilon>0$. Which is valid by the definition of $\lambda^*$ and Lemma 2.4. 
We then use the Carathéodory Criterion for our open cover over the interval:
	$\lambda^*(I)=\lambda^*\left(I \cap I_a\right)+\lambda^*\left(I \cap I_a^c\right)$.
To see that
	$\lambda^*(E) \geq$ $\sum_{I \in \mathcal{I}(E)} \lambda^*\left(I \cap I_a\right)+\lambda^*\left(I \cap I_a^c\right)-\varepsilon$
Which in turn is larger than:
	$\lambda^*\left(E \cap I_a\right)+\lambda^*\left(E \cap I_a^c\right)-\varepsilon$.
By letting $\varepsilon$ become arbitrarily close to zero we obtain the desired equality.

We can now combine these results to arrive at the following theorem:

### Theorem 2.6: Uniqueness of Lebesgue Measure on $\mathcal{B}$
The (restricted) function $\lambda: \mathcal{B} \rightarrow[0, \infty]$ is the unique measure on $\mathcal{B}$ that satisfies $\lambda(I)=\lambda_0(I)$.

**Proof**: We already know that $\lambda$ is a measure on the Carathéodory $\sigma$-Algebra $\Sigma_\lambda$.
By Lemma 2.5 we know that its restriction to the Borel-sets on the reals is also a measure. 
By Lemma 2.4 we also know that $\lambda(I)=\lambda_0(I)$. 
What remains to be proven is the uniqueness of $\lambda$:
Consider we have another measure $\mu$ with the same properties we listed above.
For any $a \in \mathbb{R}$ and $n \in \mathbb{N}$, we have: 
	$(-\infty, a] \cap[-n,+n] \in I$ 
meaning that every such intersection is an interval, hence:
	$\lambda((-\infty, a] \cap[-n,+n])=\mu((-\infty, a] \cap[-n,+n])$.
Because the intervals $(-\infty, a]$ form a $\pi$-system (closed under pairwise intersections) that generates $\mathcal{B}$ we get: 
	$\lambda(B \cap[-n,+n])=\mu(B \cap[-n,+n])$
For any $B\in\mathcal{B}$ and $n\in\mathbb{N}$. 
Every such set is bounded and thus measurable and in the domain of both measures.
In this argument we basically approximate every Borel set by truncating it with the compact intervals $[-n, n]$: 
	$B_n:=B \cap[-n, n]$.
Since both candidate measures $\lambda$ and $\mu$ are $\sigma$-Additive by definition we get that their limits must be equal:
	$\lambda(B)=\lim _{n \rightarrow \infty} \lambda\left(B_n\right), \quad \mu(B)=\lim _{n \rightarrow \infty} \mu\left(B_n\right)$.
	$\lambda(B)=\mu(B)$ 
Because for each $n$ they agree like we've already shown. 
We approximate any (possibly unbounded) Borel set $B$ by an increasing sequence of bounded Borel sets $B_n=B \cap[-n, n].$ 

The sets in the Carathéodory $\sigma$-Algebra $\Sigma_\lambda$ are also called the *Lebesgue-measurable* sets. 

A function $f: \mathbb{R} \rightarrow \mathbb{R}$ is called *Lebesgue-measurable* if the sets $\{f \leq c\}$ are in $\Sigma_\lambda$ for all $c \in \mathbb{R}$.

Are all subsets of $\mathbb{R}$ in $\Sigma_\lambda$? No. With the aforementioned construction of the Vitali sets we can prove this [[Measure Theoretic Probability#2.6 Exercises]]. 
This is a bit more subtle than one might think since we can show that $\Sigma_\lambda$ has the same cardinality as the power set of the reals: 
Consider the Cantor set in $[0, 1]$. By removing the "middle third" interval and then repeating this procedure for each of the remaining "outer thirds" countably many times we obtain a sequence of cantor sets $C_n$ and its limit: 
$C:=\bigcap_{n=1}^{\infty} C_n$
We immediately see that $\lambda(C)=0$. 
On the other hand, we see that $C$ must be uncountable because we can localize every number remaining in it by their ternary expansion $\sum_{k=1}^{\infty} x_k 3^{-k}$ with $x_k \in\{0,2\}$.
By completeness of $\left([0,1], \Sigma_\lambda, \lambda\right)$, every subset of $C$ has Lebesgue measure zero, and the cardinality of the power set of $C$ equals that of the power set of $[0, 1]$.

## 2.2 A general extension theorem

We can generalize the result of Theorem 2.6. In the specific instance above we basically proved that there exists a measure on a $\sigma$-Algebra (in this case on $\mathcal{B}$) that is such that its restriction to a suitable subclass of sets (the intervals) conforms to predetermined behavior. This is also valid in a more general sense.

### 2.7 Carathéodory's Extension Theorem

Let $\Sigma_0$ be an algebra on a set $S$ and let $\mu_0: \Sigma_0 \rightarrow[0, \infty]$ be finitely additive and countably subadditive. Then there exists a mesure $\mu$ defined on $\Sigma=\sigma\left(\Sigma_0\right)$ such that $\mu$ restricted to $\Sigma_0$ coincides with $\mu_0$. 
The measure $\mu$ is thus an extension of $\mu_0$, and this extension is unique if $\mu_0$ is $\sigma$-finite on $\Sigma_0$.

**Proof**: The general Idea is very similar to the proofs in the previous section, we first construct an outer measure on the power set of $S$, then show that by restricting the outer measure to the Carathéodory $\sigma$-Algebra of $S$ we obtain a proper measure. We then show that this measure is unique, concluding the proof.

First we define an outer Measure on $2^S$ like so: 
	$\mu^*(E)=\inf _{\Sigma_0(E)} \sum_{E_k \in \Sigma_0(E)} \mu_0\left(E_k\right)$,
where, like before, we take the infimum over all countable covers of $E$ with elements $E_k$ from $\Sigma_0$.
We've already shown that this is indeed a valid outer measure.

For any $E \in \Sigma_0$ we have:
	$(i)\quad\mu^*(E) \leq \mu_0(E)$.
Now, given any cover of E $\left\{E_1, E_2, \ldots\right\}$ with $E_k \in \Sigma_0$, we have 
	$\mu_0(E) \leq \sum_k \mu_0\left(E \cap E_k\right)$,
because \mu_0$ is countably subadditive and any set $E$ can be split into a countable union $E=\cup_k\left(E \cap E_k\right)$.
Because $\mu_0$ is also finitely additive we obtain: 
	$\mu_0\left(E \cap E_k\right) \leq \mu_0\left(E_k\right)$
Which leads us, by combination with the prior results, to:
	$\mu_0(E) \leq \sum_k \mu_0\left(E_k\right)$.
Now taking the infimum over all covers of $E$ we get:
	$\mu_0(E) \leq \mu^*(E)$, for $E \in \Sigma_0$.
Combining this with $(i)$ we see that both sides must be equal $\mu_0(E)=\mu^*(E)$ and therefore $\mu^*$ must be an extension of $\mu_0$.

TODO: The two remaining steps of the proof (page 14 of MTP) 

Remark: 
Unicity of the extension fails to hold for $\mu_0$ that are not $\sigma$-finite. (Counterexample with infinite sets and the counting measure.)
### 2.6 Exercises


# 3 Measurable Functions and Random Variables

Central goal: Define random variables as *measurable* functions on a probability space and derive their most important properties.

## 3.1 General Setting

Let $(S, \Sigma)$ be a measurable space. Recall that the elements of $\Sigma$ are called the measurable sets. 
$\mathcal{B}=\mathcal{B}(\mathbb{R})$ is the collection of all the Borel sets of $\mathbb{R}$.

### Definition 3.1 Measurable Functions / Mappings
A mapping $h: S \rightarrow \mathbb{R}$ is called *measurable* if all of its preimages $h^{-1}[B]$ are contained in $\Sigma$ for all $B \in \mathcal{B}$: 
	$h^{-1}[B] \in \Sigma \quad \forall B \in \mathcal{B}$ 
One immediately notes that this always depends on the choice of $\sigma$-Algebras. This definition is analogous to the topological definition of continuity, where one defined the property of *continuity* (roughly speaking) by the fact that the preimages of all open sets of the function images are open sets themselves. 
If $S$ is a topological space with a topology $\mathcal{T}$ and if $\Sigma=\sigma(\mathcal{T})$, a measurable function $h$ is also called a *Borel* measurable function. 

Remarks on Notation:
	$\{h \in B\}$ is shorthand for $\{s \in S: h(s) \in B\}$
	$\{h \leq c\}$ is shorthand for $\{s \in S: h(s) \leq c\}$.

### Proposition 3.3: Properties of Measurable Functions $I$

Let $(S, \Sigma)$ be a measurable space and $h: S \rightarrow \mathbb{R}$.
	(i) if $\mathcal{C}$ is a collection of subsets of $\mathbb{R}$ such that they are a generator of the Borel $\sigma$-Algebra: $\sigma(\mathcal{C})=\mathcal{B}$, and if $h^{-1}[C] \in \Sigma$ for all $C \in \mathcal{C}$, then $h$ is measurable. (preimage property)
	(ii) if $\{h \leq c\} \in \Sigma$ for all $c \in \mathbb{R}$, then $h$ is measurable. (boundedness property)
	(iii) if $S$ is topological and $h$ is continuous, then $h$ is measurable with respect to the $\sigma$-Algebra generated by the open sets. In particular, any constant function is measurable. (topological definition of continuity is analogous to preimage property for measurability)
	(iv) If $h$ is measurable and another function $f: \mathbb{R} \rightarrow \mathbb{R}$ is *Borel measurable* ($\mathcal{B} / \mathcal{B}$-measurable), then $f \circ h$ is measurable as well. (composition property)

Proof omitted for brevity, but in essence we only have to show that in (i) when we construct a $\sigma$-Algebra $\mathcal{D}=\left\{B \in \mathcal{B}: h^{-1}[B] \in \Sigma\right\}$ that this is indeed equivalent to $\mathfrak{B}$. (ii) and (iii) then follow from applying (i) and (iv) follows from: Take $B \in \mathcal{B}$, then $f^{-1}[B] \in \mathcal{B}$ because f is Borel measurable. $h$ is measurable itself and thus we obtain: $(f \circ h)^{-1}[B]=h^{-1}\left[f^{-1}[B]\right] \in \Sigma$.
More interestingly, (iv) also generalizes to arbitrary compositions, but we need to keep track of the $\sigma$-Algebras we are mapping from and into.

### Proposition 3.5: Properties of Measurable Functions $II$

We have the following properties: 
	(v) The collection $\Sigma$ of all $\Sigma$-Measurable functions is a vector space and products of measurable functions are measurable aswell.
	(vi) Let $h_n$ be a sequence in $\Sigma$. Then $\inf h_n, \sup h_n, \liminf h_n, \limsup h_n$ are also in $\Sigma$,
	where we extend the range of these functions to $[-\infty, \infty]$. The set $L$, consisting of all $s\in S$ for which $\lim _n h_n(s)$ exists as a finite limit is measurable.

**Proof**: (v) If $h$ is a measurable function and $\lambda$ any real number then $\lambda h$ is also measurable. Why? By application of (ii): 
	$h$ measurable $\Longleftrightarrow \forall c \in \mathbb{R},\{h \leq c\} \in \Sigma$.
we only need to show that 
	$\forall c \in \mathbb{R},\{\lambda h \leq c\} \in \Sigma$.
So we have three cases: $\lambda>0$, $\lambda<0$, and $\lambda=0$, the latter of which is trivial since we already know that any constant function is measurable. For the other two cases we rearrange the inequality:
	$\{\lambda h \leq c\}=\left\{h \leq \frac{c}{\lambda}\right\}$. 
And observe that since $h$ is measurable, $\left\{h \leq \frac{c}{\lambda}\right\} \in \Sigma$, because $\frac{c}{\lambda}$ is just another real number.
We obtain the case for negative $\lambda$ from the equivalent characterization of measurability: 
	$\{x \in X: h(x) \leq c\} \in \Sigma$. where we can swap $\{h<c\},\{h \geq c\},\{h>c\}$ as equivalent properties for measurable functions. 
To then prove that the sum of two measurable functions remains measurable we represent the set $\{f+g \leq c\}$ for any $c \in \mathbb{R}$, as a countable union like so: 
	$\{f+g \leq c\}=\bigcup_{q \in \mathbb{Q}}(\{f \leq q\} \cap\{g \leq c-q\})$.
This holds because $\mathbb{Q}$ is dense in $\mathbb{R}$ and thus, if $f(x)+g(x) \leq c$, there exists $q \in \mathbb{Q}$ such that $f(x) \leq q$ and $g(x) \leq c-q$, yielding exactly the desired property that the sum of these parts is still less than or equal than $c$.
This means that we can represent the preimages of $\{f+g \leq c\}$ as a countable union of intersections, which are operations that will always yield measurable sets. 
Now all that remains to be shown is that the products of measurable functions are measurable:
	If $f$ and $g$ are measurable then $f \cdot g$ is measurable.
We use an algebraic trick to represent this product as a linear combination of squares:
	$f \cdot g=\frac{1}{4}\left[(f+g)^2-(f-g)^2\right]$.
We've already shown that sums, and by symmetry differences, preserve measurability. 
Now we can prove the fact that for any measurable function $h$ squaring preserves measurability analogously to our approach for sums: 
	$\forall c \in \mathbb{R}, \quad\left\{h^2 \leq c\right\} \in \Sigma$ because:
	If $c<0$, then $\left\{h^2 \leq c\right\}=\emptyset$ (which is measurable).
	For $c \geq 0$, observe that:
	$\left\{h^2 \leq c\right\}=\{-\sqrt{c} \leq h \leq \sqrt{c}\}$.
	Or equivalently: 
	$h^2 \leq c \Longleftrightarrow-\sqrt{c} \leq h \leq \sqrt{c}$, 
	where $\{h \leq \sqrt{c}\}$ and $\{h \geq-\sqrt{c}\}$ are both measurable as we've already established. 
	Therefore $\left\{h^2 \leq c\right\}=\{h \leq \sqrt{c}\} \cap\{h \geq-\sqrt{c}\}$ is measurable as intersection of measurable sets.
Combining all three parts of this proofs yields the desired result: The collection $\Sigma$ of $\Sigma$-measurable functions is a vector space. This also concludes exercise 3.1 $\square$

For the proof of statement (vi) we simply use the fact that the infimum can be represented as a countable intersection of measurable sets, which can be applied to supremum and limsup and liminf respectively. 

### Theorem 3.6: Monotone Class Theorem

Let $\mathcal{H}$ be a vector space of bounded functions with the properties: 
	 (i) $1 \in \mathcal{H}$.
	 (ii) If $\left(f_n\right)$ is a nonnegative sequence in $\mathcal{H}$ such that $f_{n+1} \geq f_n$ for all $n$ and $f:=\lim f_n$ bounded, then $f \in \mathcal{H}$.
If, in addition, $\mathcal{H}$ contains the indicator functions of sets in a $\pi$-system $\mathcal{I}$ then $\mathcal{H}$ contains all bounded $\sigma(\mathcal{I})$-measurable functions. 

Intuition: $\mathcal{H}$ is a vector space e.g. closed under linear combinations (i) **and** it is closed under **bounded pointwise monotone limits** (ii).
This is useful because often it is easier to check closure under monotone limits than it is to verify closure under countable unions, intersections, and complements directly. With this theorem we can "bootstrap" a small class of sets/functions up to the entire $\sigma$-Algebra. 
* MCT allows us to prove that certain operations preserve measurability
* MCT allows us to extend properties verified for simple functions to all measurable functions
(We will define *simple* functions later)
This is very useful for later theorems like Monotone Convergence Theorem, Dominated Convergence Theorem, proving closure properties of measurable functions etc.

## 3.2 Random Variables

We now consider a set of outcomes $\Omega$ and a $\sigma$-Algebra $\mathcal{F}$ of events defined on it. In this setting Definition 3.1 (Measurable Functions/Mappings) takes the following form:

### Definition 3.7: Random Variable
A function $X: \Omega \rightarrow \mathbb{R}$ is called a *random variable* if it is $(\mathcal{F})$-measurable. 

We denote random variables by capital letters. By definition, random variables are nothing but measurable functions with respect to a given $\sigma$-Algebra $\mathcal{F}$.

Given 
	$X: \Omega \rightarrow \mathbb{R}$, 
let 
	$\sigma(X)=\left\{X^{-1}[B]: B \in \mathcal{B}\right\}$.
Then 
	$\sigma(X)$ is a $\sigma$-Algebra and $X$ a random variable in the sense of definition 3.7 iff $\sigma(X) \subset \mathcal{F}$.
(The $\sigma$-Algebra generated by a random variable.)
The proof is pretty straight forward, we already know by construction that $\sigma(X)$ is a $\sigma$-Algebra the minimal property is also easy to verify. 
This relates to Exercise 3.2 as we can show that the pi-system $\Pi(X):=\{\{X \leq x\}: x \in \mathbb{R}\}$ generates the Borel sigma algebra. The cumulative distribution function of a random variable X is defined as:
	$F_X(x):=P(X \leq x)=P(\{\omega: X(\omega) \leq x\})$.
Since $\{X \leq x\}$ are precisely the sets in the pi-system that generates $\sigma(X)$ the entire distribution of $X$ is determined by the probabilities of these threshold sets.

### Proposition 3.8: Distribution Function of a Random Variable
The *distribution function* of a random variable is the function: 
	$F: \mathbb{R} \rightarrow[0,1]$, given by $F(x) = \mu((-\infty, x])=\mathbb{P}(X \leq x)$
It is: 
	(i) right coninuous
	(ii) non-decreasing
	(iii) satisfies: $\lim _{x \rightarrow \infty} F(x)=1$ and $\lim _{x \rightarrow-\infty} F(x)=0$.
	(iv) has at most countably many discontinuities

Proof idea for (iv): We know that F is bounded from above and below. Each discontinuity must result in a positive jump > 0. We can only fit countably many such jumps into $[0, 1]$.

### Proposition 3.9: Distribution Functions Fully Describe Probability Distributions

Let $\mu_1$ and $\mu_2$ be probability measures on $\mathcal{B}$. Let $F_1$ and $F_2$ be corresponding distribution functions. If $F_1(x) = F_2(x)$ for all $x$, then $\mu_1 = \mu_2$.
**Proof:** Again, we refer to exercise 3.2 where we prove that the $\pi$-System $\mathcal{I}=\{(-\infty, x]: x \in \mathbb{R}\}$ generates the Borel-$\sigma$-Algebra and invoke Theorem [[Measure Theoretic Probability#Theorem 1.15 Uniqueness of Measures]].

Therefore we have shown, that for any random variable $X$, its distribution, as in the *collection of all probabilities* $\mathbb{P}(X \in B)$ with $B \in \mathcal{B}$, is fully determined by its respective distribution function $F_X$.

We call *any* function on $\mathbb{R}$ with the properties given in Proposition 3.8 a distribution function. Any distribution function is measurable (by their non-decreasing property and the fact that all sets $\{F \geq c\}$ are intervals and thus in $\mathcal{B}$).
We will show in theorem 3.10 that for any distribution function $F$,  it is possible to construct a random variable on some probability space $(\Omega, \mathcal{F}, \mathbb{P})$, whose distribution function equals F. This theorem is based on the existence of the Lebesgue measure $\lambda$ on the Borel sets $\mathcal{B}[0,1]$ of $[0,1]$.
[[Measure Theoretic Probability#Theorem 1.5 Existence and uniqueness of Lebesgue Measure]].
What does this mean in practice? 

Consider: $(\Omega, \mathcal{F}, \mathbb{P})=([0,1], \mathcal{B}[0,1], \lambda)$. Let $U: \Omega \rightarrow[0,1]$ be the identity map. 
The distribution function $F^U$ of $U$ satisfies
	$F^U(x) = x$ for $x \in[0,1]$ and so 
	$\mathbb{P}(a<U \leq b)=F^U(b)-F^U(a)=b-a$ for $a, b \in[0,1]$ with $a \leq b$.
Hence the distribution function $F^U$ corresponds to a probability measure on $([0,1], \mathcal{B}[0,1])$ and there exists a random variable $U$ on this space such that $U$ has $F^U$ as its distribution function. 
This random variable is said to have the standard uniform distribution. 

### Theorem 3.10: Skorokhod's Representation Theorem
Let F be a distribution function on $\mathbb{R}$. Then there exists a probability space and a random variable $X: \Omega \rightarrow \mathbb{R}$ such that $F$ is the distribution function of $X$.

**Proof:** Exercise 3.6 For the continuous and strictly increasing case

## 3.3 Independence

In basic probability theory we defined pairwise independence of evens $E, F \in \mathcal{F}$ by the product rule: $\mathbb{P}(E \cap F)=\mathbb{P}(E) \mathbb{P}(F)$. 
We now want to generalize this notion of independence to independence of sequences of events and independence of sequences of $\sigma$-Algebras. 

### Definition 3.11: Independence
Independence for $\sigma$-Algebras, random variables, and events:
	(i) A sequence of $\sigma$-algebras $\mathcal{F}_1, \mathcal{F}_2, \ldots$ is called independent, if for every $n$ it holds that $\mathbb{P}\left(E_1 \cap \cdots \cap E_n\right)=\prod_{i=1}^n \mathbb{P}\left(E_i\right)$, for all choices $E_i \in \mathcal{F}_i$ $(i=1, \ldots, n)$.
	(ii) A sequence of random variables $X_1, X_2, \dots$ is called independent if the $\sigma$-algebras $\sigma\left(X_1\right), \sigma\left(X_2\right), \ldots$ are independent. 
	(iii) A sequence of events $E_1, E_2, \ldots$ is called independent if th erandom variables $\mathbf{1}_{E_1}, \mathbf{1}_{E_2}, \ldots$ are independent. 

This definition also applies to finite sequences. If follows that two $\sigma$-Algebras $\mathcal{F}_1$ and $\mathcal{F}_2$ are independent if:
	$\mathbb{P}\left(E_1 \cap E_2\right)=\mathbb{P}\left(E_1\right) \mathbb{P}\left(E_2\right)$ for all $E_1 \in \mathcal{F}_1$ and $E_2 \in \mathcal{F}_2$.
To check the independence of two $\sigma$-Algebras, theorem 1.15 is helpful once again:
### Proposition 3.12 Independence of $\sigma$-Algebras
Let $\mathcal{I}$ and $\mathcal{J}$ be $\pi$-systems and suppose that for all $I \in \mathcal{I}$ and $J \in \mathcal{J}$ the product rule $\mathbb{P}(I \cap J)=\mathbb{P}(I) \mathbb{P}(J)$ holds. Then the $\sigma$-algebras $\sigma(\mathcal{I})$ and $\sigma(\mathcal{J})$ are independent.

**Proof**: Let $\mathcal{G}=\sigma(\mathcal{I}) \text { and } \mathcal{H}=\sigma(\mathcal{J}) .$ 
We define on each $I \in \mathcal{I}$ the *finite measures* $\mu_I$ and $\nu_I$ on $\mathcal{H}$ by $\mu_I(H)=\mathbb{P}(H \cap I)$ and $\nu_I(H)=\mathbb{P}(H) \mathbb{P}(I)(H \in \mathcal{H})$.
When do these measures coincide? 
The product rule holds and since they are both $\pi$-systems this holds in the finite case.
(pi-systems closed under intersections).
Assuming now that $\mu_I(\Omega)=\mathbb{P}(I)=\nu_I(\Omega)$, we see that $\mu_I$ and $\nu_I$ coincide on $\mathcal{J}$.
Using theorem 1.15 we know that they must coincide for all H: $\mu_I(H)=\nu_I(H)$ for all $H \in \mathcal{H}$.
Now repeating these steps with swapped finite measures $\mu^H$ and $\nu^H$: $\mu^H(G)=\mathbb{P}(G \cap H)$ and $\nu^H(G)=\mathbb{P}(G) \mathbb{P}(H)$. We get the analogous result for $\mathcal{I}$. $\quad \square$

Why is this important? 
$\sigma$-Algebras are not easy to work with and can be quite abstract like we outlined in earlier discussions. Proposition 3.12 allows us to prove independence on the generating systems of sets that are in most cases much more manageable. 
A simple example (that kind of seems circular because we use i.i.d. to use the product rule to prove independence): 
Suppose we have i.i.d. random variables $X_1, X_2, \ldots, X_n$.
- The sigma-algebra generated by $X_i$ is $\sigma\left(X_i\right)$.
- This is generated by sets of the form $\left\{X_i \in A_i\right\}$, where $A_i \in \mathcal{B}(\mathbb{R})$.
Now we can form a $\pi$-System like so: 
	$\mathcal{I}_i:=\left\{\left\{X_i \in A_i\right\}: A_i \in \mathcal{B}(\mathbb{R})\right\} .$
And only have to prove:
	$\mathbb{P}\left(X_1 \in A_1, X_2 \in A_2, \ldots, X_n \in A_n\right)=\prod_{i=1}^n \mathbb{P}\left(X_i \in A_i\right) .$ 
In order to verify that the underlying $\sigma$-Algebras and by extension random variables are independent. Better examples will follow.

### Corollary 3.13
Let $X_1, X_2$ be random variables defined on some probability space $(\Omega, \mathcal{F}, \mathbb{P})$.
Then $X_1$ and $X_2$ are independent iff:
$\mathbb{P}\left(\left\{X_1 \leq x_1\right\} \cap\left\{X_2 \leq x_2\right\}\right) = \mathbb{P}\left(X_1 \leq\right. \left.x_1\right) \mathbb{P}\left(X_2 \leq x_2\right)$ for all $x_1, x_2 \in \mathbb{R}$.

**Proof:** Combining proposition 3.12 with exercise 3.2.

### Lemma 3.14: Borel-Cantelli

(i) Let $\left(E_n\right)_{n \geq 1}$ be a sequence of events in a probability space $(\Omega, \mathcal{F}, \mathbb{P})$.
- Suppose:

$$
\sum_{n=1}^{\infty} \mathbb{P}\left(E_n\right)<\infty
$$

- Then:

$$
\mathbb{P}\left(\limsup _{n \rightarrow \infty} E_n\right)=0,
$$

i.e., with probability 1 , only finitely many of the events $E_n$ occur.
If the total sum of the event probabilities is finite then the evens cannot keep happening forever.
This is (intuitively) similar to the proof that a distribution function can only have countably many discontinuities: Every time an event happens, it requires some non-zero amount of probability. If the "total available probability" is finite, then the total number of events that happen must be finite.

(ii) Now suppose that the events $E_n$ are independent, and

$$
\sum_{n=1}^{\infty} \mathbb{P}\left(E_n\right)=\infty .
$$


Then:

$$
\mathbb{P}\left(\limsup _{n \rightarrow \infty} E_n\right)=1,
$$

i.e., with probability 1 , infinitely many of the events happen.
If the events are independent, and there is "enough probability mass" overall (the sum diverges), then the system cannot avoid having infinitely many hits. 

## Exercises for Chapter 3

# 4. Integration

The goal of this Chapter is a rigorous construction of an integration functional that generalizes the idea of the Riemann integral from calculus. We will show how to "build up" towards an integration functional from *simple functions*, proving some of their key properties and how to extend these ideas to functions that can be expressed as combinations of simple functions. This will help us to develop the Expectation operator, unifying the idea of discrete and continuous random variables and their Expectations in the process. This will also help us compute expectations that are, at first glance, hard to pin down, such as for mixed random variables. 

## 4.1 Integration of Simple Functions
Recall how in the case of the Riemann integral one approximates the "area under the curve" by calculating the upper and lower rectangle sums. In the case they coincide we say that the integral is well defined. It is natural to define the integral of a multiple of an indicator function as 
	$a \cdot \mathbf{1}_E$ as $a \cdot \mu(E)$, for $E \in \Sigma$.
We can use this idea to define the class of *simple functions*:
### 4.1 Definition of Simple Functions
A function $f: S \rightarrow[0, \infty)$ is called a nonnegative simple function, if it has a representation as a finite sum: 
	$f=\sum_{i=1}^n a_i \mathbf{1}_{A_i}$
where $a_i \in[0, \infty)$ and $A_i \in \Sigma$. Basically, if we are able to split the function into a countable number of constant parts, the sum of the respective multiples of the indicator functions is what we refer to as a *simple function*. We denote the class of all nonnegative simple functions by $\mathfrak{S}^{+}$.
We directly see that simple functions, as linear combination of indicator functions, are **measurable**.

Since we remember that Riemann integrals are linear operators and are equipped with the definition of the integral for an indicator function we can directly define an integral for any $f \in \mathfrak{S}^{+}$.

### Definition 4.2: The Lebesgue Integral for Simple Functions
Let $f \in \mathfrak{S}^{+}$. The (Lebesgue) integral of $f$ with respect to the measure $\mu$ is defined as: 
	$\int f \mathrm{~d} \mu:=\sum_{i=1}^n a_i \mu\left(A_i\right)$,
When $f$ has a representation as defined in 4.1.
We see that the integral functional is nothing but a sum of set functions over the respective preimages, namely the sizes $\mu\left(A_i\right)$ of every preimage mapping to the corresponding image $a_i$.
Other common notations that are often used for this integral are $\int f(s) \mu(\mathrm{d} s)$ and $\mu(f)$.
Note that if $f=\mathbf{1}_A$, then $\mu(f)=\mu\left(\mathbf{1}_A\right)=\mu(A)$, so there is a bit of ambiguity in the notation.
Note that $\mu(f) \in[0, \infty]$ and also that the above summation is well defined, since all quantities involved are nonnegative, although possibly infinite. 

It should be clear that this definition of integral is, at first sight, troublesome. The representation of a simple function is not unique, and one might wonder if the integral defined as such may take on different values for different representations. This would be bad, but fortunately it is not the case.

### Proposition 4.3: Representation Invariance for the Lebesgue Integral
Let $f$ be a nonnegative simple function. Then the value of the integral $\mu(f)$ is independent of the chosen representation. 

**Proof**: We want to show that for any possible representations of $f$:
	$f=\sum_{i=1}^n a_i \mathbf{1}_{A_i}=\sum_{j=1}^m b_j \mathbf{1}_{B_j}$
The value of the integral remains the same, no matter which representation we pick. 
We want to show: 
	$\sum_{i=1}^n a_i \mu\left(A_i\right)=\sum_{j=1}^m b_j \mu\left(B_j\right)$
Where $A_i$ 's and $B_j$ 's form measurable sets and $A_i$ are pairwise disjoint and $a_i$ nonzero, and $B_j$ are pairwise disjoint and $b_j$ nonzero.
Step 1: Construct a common refinement:
Consider the collection of all pairwise intersections $C_{i j}=A_i \cap B_j$. Then the set of all $C_{ij}$ forms a measurable partition of $\Omega$, where some of them may possibly be empty sets. 
If now any $x \in B_k$ then $x$ must be contained in one of the $A_j$ because otherwise their respective indicator functions would be zero, contradicting the definition for our representations. 
Therefore $B_j \subset \cup A_i$  such that $\mu(B_j)$ is finite for all k and therefore our representation $B_j$ is integrable.
By the same reasoning we obtain:
	$B_k=\cup_j\left(A_j \cap B_k\right)$ and $A_j=\cup_k\left(B_k \cap A_j\right)$
If $\boldsymbol{A}_j \cap \boldsymbol{B}_k$ is not empty, then for any $x \in A_j \cap B_k$ we have
	$a_j=s_A(x)=s_B(x)=b_k$
Combining this and swapping the order of summation we get the following: 
$$
\int s_B(x) d x=\sum_{k=1}^m b_k m\left(B_k\right)=\sum_{k=1}^m b_k m\left(\bigcup_{j=1}^n A_j \cap B_k\right)=
$$
$$
=\sum_{k=1}^m b_k \sum_{j=1}^n m\left(A_j \cap B_k\right)=\sum_{j=1}^n \sum_{k=1}^m b_k m\left(A_j \cap B_k\right)=
$$
$$
=\sum_{j=1}^n \sum_{k=1}^m a_j m\left(A_j \cap B_k\right)=\sum_{j=1}^n a_j \sum_{k=1}^m m\left(A_j \cap B_k\right)=
$$
$$
=\sum_{j=1}^n a_j m\left(A_j\right)=\int s_A(x) d x
$$
This concludes the proof that the integrals agree regardless of the chosen representation. 
It remains to be shown that the integral of a simple function agrees with the integral over the canonical representation of that function, i.e. when the sets are disjoint and coefficients are nonzero. 
For this step we get back to our sets $C_i$:
Recall that any simple function $f$ can be written in its canonical form:
	$f=\sum_{i=1}^n c_i \mathbf{1}_{C_i}$
Where $C_i$ are disjoint measurable sets, $c_i$ are distinct, nonzero values that $f$ takes, and the sets are chosen such that $C_i=\left\{x \in \Omega: f(x)=c_i\right\}$. This canonical form is unique up to $\mu$-nullsets.
We observe that any representation can be refined into this canonical representation by considering: 
	$C_i=\left\{x \in \Omega: f(x)=c_i\right\}=\bigcup_{j: a_j=c_i} A_j$
Using the first part of our proof we then obtain:
$$
\sum_j a_j \mu\left(A_j\right)=\sum_{i=1}^n c_i \sum_{j: a_j=c_i} \mu\left(A_j\right)=\sum_{i=1}^n c_i \mu\left(\bigcup_{j: a_j=c_i} A_j\right)=\sum_{i=1}^n c_i \mu\left(C_i\right)
$$
Analogous to our the previous steps. Thus we have shown that the Lebesgue integral for simple functions is invariant of the chosen representation, concluding the proof. $\quad\square$

This is technically all we need to define an integration functional for discrete random variables, since we already know that random variables only take at most countably many values: 
Define $A_i=\left\{\omega \in \Omega: X(\omega)=x_i\right\}$, then $X(\omega)=\sum_i x_i \mathbf{1}_{A_i}(\omega)$, which leads us to the integral: 
	$\int X d \mathbb{P}=\sum_i x_i \mathbb{P}\left(A_i\right)$
Which is precisely the definition of the Lebesgue Integral of a simple function. For the continuous case we do need to expand our toolbox a bit more however.

### Example 4.6: Lebesgue Integrable Function that is not Riemann integrable
Let $(S, \Sigma, \mu)=([0,1], \mathcal{B}([0,1]), \lambda)$ where $f$ is the indicator function of the rational numbers in $[0, 1]$:
$f=\mathbf{1}_{\mathbb{Q} \cap[0,1]}$. We know that $\lambda(\mathbb{Q} \cap[0,1])=0$ and thus it follows that $\lambda(f)=0$. This is a nice example of a function that is not Riemann integrable, since the upper and lower limits would not converge to the same number, whereas it is Lebesgue integrable trivially and has a very sensible solution. 
Refresher: Why is $\lambda(\mathbb{Q} \cap[0,1])=0$ ? 
Two arguments: 
- The rational numbers are countable, and the measure is countably additive / $\sigma$-additive, meaning that, because the measure of singleton sets is zero, the total measure of all rationals in the interval must also be zero. 
- Given any $\varepsilon>0$, for each rational $q_i \in[0,1]$, we construct an open interval $I_i$ centered at $q_i$ of length $\ell\left(I_i\right)=\varepsilon / 2^i$. The total length of the cover is therefore $\sum_{i=1}^{\infty} \frac{\varepsilon}{2^i}=\varepsilon$. Since $\varepsilon>0$ is arbitrarily small, the Lebesgue outer measure of $\mathbb{Q} \cap[0,1]$ must therefore be arbitrarily small and thus $\lambda(\mathbb{Q} \cap[0,1])=0$.

### Proposition 4.7: Properties of Simple Functions
We say that a property of elements of S holds almost everywhere, if the set for which this property does not hold, has measure zero. For instance we say that two measurable functions are equal almost everywhere if $\mu(\{f \neq g\})=0$. Elementary properties of the integral are listed below:
Let $f, g \in \mathfrak{S}^{+}$and $c \in[0, \infty)$.
	(i) If $f \leq g$ a.e., then $\mu(f) \leq \mu(g)$.
	(ii) If $f=g$ a.e., then $\mu(f)=\mu(g)$.
	(iii) $\mu(f+g)=\mu(f)+\mu(g)$ and $\mu(c f)=c \mu(f)$.

We omit the proofs here, as they can be done by constructing measurable partitions and summing up carefully. 

## 4.2 A General definition of the Integral

How can we extend the ideas in the previous section to a more general class of functions? A direct approach is to expand definition 4.1 with the supremum, which allows us to recover all previous results in the case of simple functions, but also leads to many more powerful results.

### Definition 4.8 Integrals for Nonnegative Measurable Functions

Let $f$ be a nonnegative *measurable* function. The integral of $f$ is defined as:
$$
\mu(f):=\sup \left\{\mu(h): h \leq f, h \in \mathfrak{S}^{+}\right\},
$$
where $\mu(h)$ follows definition 4.2. As introduced above, this recovers the integral for simple functions completely.

### Proposition 4.9: Extending Integral Results to Measurable Functions
Let $f, g \in \Sigma^{+}$. If $f=0$ a.e., then $\mu(f)=0$. If $f \leq g$ a.e., then $\mu(f) \leq \mu(g)$, and if $f=g$ a.e., then $\mu(f)=\mu(g)$. 
**Proof**:
Part 1: Let $f \in \Sigma^+$, $f = 0 a.e.$ Then, using Proposition 4.7, we can take any Take $h \in \mathfrak{S}^{+}$with $h \leq f$, conforming to our supremum definition of $\mu(f)$, and obtain $\{h>0\} \subset\{f>0\}$, and thus $\mu(\{h>0\}) \leq \mu(\{f>0\})$. The right side of this inequality is zero since f is only nonzero on $\mu$-nullsets and hence $h=0$ a.e. By 4.7(ii) $\mu(h) = 0$ and therefore $\mu(f)$, being the supremum of those $\mu(h)$, must also be zero. 
Part 2: 
Let $f, g \in \Sigma^{+}$and $N=\{f>g\}$. Take $h \in \mathfrak{S}^{+}$with $h \leq f$.
Then we know that $h \mathbf{1}_N, h \mathbf{1}_{N^c} \in \mathfrak{S}^{+}$ and by proposition 4.7 (iii), and the fact that $h \mathbf{1}_N=0$ a.e., we have $\mu(h)=\mu\left(h \mathbf{1}_N\right)+\mu\left(h \mathbf{1}_{N^c}\right)=\mu\left(h \mathbf{1}_{N^c}\right)$. (We can always split a measurable function into the parts that are $\mu$-measurable and the "remaining" $\mu$-nullsets.) Moreover we see that:
	$h 1_{N^c} \leq f 1_{N^c} \leq g 1_{N^c} \leq g$
Because we have defined $\mu$ as supremum we obtain $\mu(h) \leq \mu(g)$.
By taking the supremum in this inequality over all $h$, we get $\mu(f) \leq \mu(g)$. The equality assertion follows analoguously.

### Lemma 4.11: The Zero Lemma for NN-Measurable Functions
Let $f \in \Sigma^{+}$and suppose that $\mu(f)=0$. Then $f=0$ a.e.

This already makes intuitive sense since all possible contributions to the integral can only be null or positive, since $f$ is a nonnegative measurable function. If there were any positive contribution however, we would not be able to "decrease" the total integral to zero anymore. Thus all $\mu$-measurable contributions to the integral must be zero e.g. the $f$ must be equal to 0 almost everywhere. Here is a rigorous proof to back up this intuition: 

**Proof**:
Because $\mu(f)=0$, it holds that $\mu(h)=0$ for all nonnegative simple functions with $h \leq f$.
Take $h_n=\frac{1}{n} 1_{\{f \geq 1 / n\}}$, then $h_n \in \mathfrak{S}^{+}$and $h_n \leq f$.
The equality $\mu\left(h_n\right)=0$ implies $\mu(\{f \geq 1 / n\})=0$. The desired result follows then from $\{f>0\}=\cup_n\{f \geq 1 / n\}$ and [[Measure Theoretic Probability#Corrolary 1.8 $ sigma$-sub-Additivity]].
The idea is to show that any measurable set where $f$ is at least some positive amount 1/n must have measure zero. We do this by defining our $h_n$ strictly less than or equal to $f$ and can use monotonicity of the integral to show that the resulting integral must be less than or equal to zero. 
We then use the countable union property to conclude that f must indeed be zero almost everyhwere.

### 4.12 Monotone Convergence Theorem
Let $\left(f_n\right)$ be a sequence in $\Sigma^{+}$, such that $f_{n+1} \geq f_n$ a.e. for each $n$. Let $f=\limsup f_n$. Then $\mu\left(f_n\right) \uparrow \mu(f) \leq \infty$.

This is so useful because it allows us to swap limits and integration under some rather general conditions.  
We skip the proof here as it is provided in the source material, and go directly to an application of MCT

### 4.13 Application of MCT (Canonical Simple Approximation)
Let $f \in \Sigma^{+}$ and, for each 
$n \in \mathbb{N}$, put $E_{n, i}=\left\{i 2^{-n} \leq f<(i+1) 2^{-n}\right\}\left(i \in I_n:=\left\{0, \ldots, n 2^n-1\right\}\right)$.
Put also $E_n=\{f \geq n\}$ and note that the sets $E_{n, i}$ and $E_n$ are in $\Sigma$.
Define now
	$f_n=\sum_{i \in I_n} i 2^{-n} \mathbf{1}_{E_{n, i}}+n \mathbf{1}_{E_n}$.
These $f_n$ form an increasing sequence in $\Sigma^+$, even in $\mathfrak{S}^{+}$, with limit $f$.
We have thus constructed a general sequence of simple functions with limit $f$, that can be used to approximate $\mu(f)$.

### Proposition 4.14: Linearity of the Integral for NN-measurable Functions
Let $f, g \in \Sigma^{+}$and $\alpha, \beta>0$. Then $\mu(\alpha f+\beta g)=\alpha \mu(f)+$ $\beta \mu(g) \leq \infty$.

### 4.15 Fatou's Lemma
The Basic Idea behind the MCT, Fatou's Lemma, and the Dominated Convergence Theorem is that given a sequence of functions $f_n$ that converge pointwise to some limit function $f$, it is *not always* true that 
	$\int \lim _{n \rightarrow \infty} f_n=\lim _{n \rightarrow \infty} \int f_n$
Fatou's Lemma, MCT, and DCT all answer the question "When do $\lim _{n \rightarrow \infty}$ and $\int$ commute?"
The MCT and DCT tell us that as long as we place certain restrictions on both $f_n$ and $f$, then we can interchange the limit and integral signs. Fatou's Lemma is a result that tells us the best we can do if we *do not* place any restrictions on the behavior of the functions. 

**Fatou's Lemma**: Let $(X, \Sigma, \mu)$ be a measurable space and $\left\{f_n: X \rightarrow[0, \infty]\right\}$ a sequence of nonnegative, measurable functions. Then the function $\liminf _{n \rightarrow \infty} f_n$ is measurable and 
$$
\int_X \liminf _{n \rightarrow \infty} f_n d \mu \leq \liminf _{n \rightarrow \infty} \int_X f_n d \mu
$$
**Proof**:
For each $k \in \mathbb{N}$, let $g_k=\inf _{n \geq k} f_n$ and define 
	$$
h=\lim _{k \rightarrow \infty} g_k=\lim _{k \rightarrow \infty} \inf _{n \geq k} f_n=\liminf _{n \rightarrow \infty} f_n
$$
From the definition of the infimum we observe that $\int g_k \leq \int f_n$ for all $n \geq k$. Hence $\int \inf _{n \geq k} f_n \leq \int f_n$ for all $n \geq k$ as claimed. This allows us to write 
$$
\int g_k \leq \inf _{n \geq k} \int f_n.
$$
We then observe that $g_k$ is an increasing sequence (because the infimum cannot get smaller by removing sequence elements) and $\lim _{k \rightarrow \infty} g_k=h$ pointwise. 
Applying the MCT like so: 
$$
\int \liminf _{n \rightarrow \infty} f_n= \int h=\lim _{k \rightarrow \infty} \int g_k \leq \lim _{k \rightarrow \infty} \inf _{n \geq k} \int f_n=\liminf _{n \rightarrow \infty} \int f_n
$$
where the inequality in the middle follows from (1). We arrive at the desired result. 
We conclude the proof by noting that $\liminf f_n$ is measurable as we've already shown. $\quad \square$ 

**Example from Rudin:** 
Define $$
f_n= \begin{cases}\mathbf{1}_{(1,2]} & \text { if } n \text { is even } \\ \mathbf{1}_{[0,1]} & \text { if } n \text { is odd }\end{cases}
$$
As n increases the graph of $f_n$ switches back and forth from the indicator over each interval.
For any given $n$, we obtain the same integral
$$
\int_{[0,2]} f_n=1
$$
But $\liminf _n f_n=0$, because $\liminf _n f_n$ is the infimum over all subsequential limits of $f_n$. This directly leads us to
$$
0=\int_{[0,2]} \liminf _{n \rightarrow \infty} f_n<\liminf _{n \rightarrow \infty} \int_{[0,2]} f_n=1,
$$ proving that a strict inequality in Fatou's lemma is possible.

Can we extend this notion of integral to (almost) arbitrary measurable functions? 

### Definition 4.17: Positive & Negative Parts of Functions
Let $f \in \Sigma$. For (extended) real numbers $x$ we define $x^{+}=\max \{x, 0\}$ and $x^{+}=\max \{x, 0\}$.
For $f: S \rightarrow[-\infty, \infty]$, we define the functions $f^{+}$ and $f^{-}$ by $f^{+}(s)=f(s)^{+}$and $f^{-}(s)=f(s)^{-}$.
This allows us to decompose any nonnegative measurable function into two parts like so: 
	$f=f^{+}-f^{-}$
Why is this relevant? Because we immediately see from this that both the positive and negative part of our initial *arbitrary* function fall into the class of *nonnegative* measurable functions again.
	If $f \in \Sigma$, then $f^{+}, f^{-} \in \Sigma^{+}$.
With this trick we can thus extend the integral properties we've derived for nonnegative measurable functions to arbitrary measurable functions:

Let $f \in \Sigma$ and assume that $\mu\left(f^{+}\right)<\infty$ or $\mu\left(f^{-}\right)<\infty$. Then we define $\mu(f):=\mu\left(f^{+}\right)-\mu\left(f^{-}\right)$.
If both $\mu\left(f^{+}\right)<\infty$ and $\mu\left(f^{-}\right)<\infty$, we say that $f$ is *integrable*.
The set of all integrable functions is denoted by $\mathcal{L}^1(S, \Sigma, \mu)$.
Note that $f \in \mathcal{L}^1(S, \Sigma, \mu)$ implies that $|f|<\infty \mu$-a.e.

### Proposition 4.18: Properties of the Integral
(i) Let $f, g \in \mathcal{L}^1(S, \Sigma, \mu)$ and $\alpha, \beta \in \mathbb{R}$. Then $\alpha f+\beta g \in \mathcal{L}^1(S, \Sigma, \mu)$ and $\mu(\alpha f+\beta g)=\alpha \mu(f)+\beta \mu(g)$. Hence $\mu$ can be seen as a linear operator on $\mathcal{L}^1(S, \Sigma, \mu)$.
(ii) If $f, g \in \mathcal{L}^1(S, \Sigma, \mu)$ and $f \leq g$ a.e., then $\mu(f) \leq \mu(g)$.
(iii) Triangle inequality: If $f \in \mathcal{L}^1(S, \Sigma, \mu)$, then $|\mu(f)| \leq \mu(|f|)$.

Proof: Exercise 4.3

This concludes the "Standard Machinery" of integration theory, whereby one often proves results by starting out with indicator functions, extending by linearity to nonnegative simple functions. After this one invokes the MCT, extending results to the class of nonnegative measurable functions. Now one shows the results to be true for functions in $\mathcal{L}^1(S, \Sigma, \mu)$ by splitting arbitrary measurable functions into positive and negative parts. This is also known as measure theoretic induction.

We will now conclude this section by the aforementioned Dominated Convergence Theorem.

### Theorem 4.19: Dominated Convergence Theorem
