https://www.semanticscholar.org/paper/Long-range-Sequence-Modeling-with-Predictable-Zhuang-Zhang/01d08fa6c229bf3070600e49f8ab05449361817e

https://www.semanticscholar.org/paper/Fast-Transformers-with-Clustered-Attention-Vyas-Katharopoulos/cd4ffe5e014601a3d6b64121355d29a730591490

https://www.semanticscholar.org/paper/Predicting-Attention-Sparsity-in-Transformers-Treviso-G'ois/d291149a75d7ac194382bd61e515eb40ed0aa106

https://www.semanticscholar.org/paper/%24O(n)%24-Connections-are-Expressive-Enough%3A-Universal-Yun-Chang/40ca4fcfffa7ca9aa9b7ff06ecf3cd0436712d78

https://www.semanticscholar.org/paper/Random-Feature-Attention-Peng-Pappas/9ed25f101f19ea735ca300848948ed64064b97ca

https://www.semanticscholar.org/paper/Fast-Attention-Over-Long-Sequences-With-Dynamic-Pagliardini-Paliotta/5b94dd68c7484d828bb3ed4b065fc58ddbd3b202

https://www.semanticscholar.org/paper/Fourier-Transformer%3A-Fast-Long-Range-Modeling-by-He-Yang/c9a69319c56c897580c18eb3b6187b465e872b76

https://www.semanticscholar.org/paper/Informer%3A-Beyond-Efficient-Transformer-for-Long-Zhou-Zhang/5b9d8bcc46b766b47389c912a8e026f81b91b0d8

https://www.semanticscholar.org/paper/Switch-Transformers%3A-Scaling-to-Trillion-Parameter-Fedus-Zoph/fdacf2a732f55befdc410ea927091cad3b791f13

