Fast and Memory-Efficient Exact Attention with IO-Awareness
https://arxiv.org/abs/2205.14135
https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad
[[Online normalizer calculations for softmax]]
[[Self-attention Does Not Need On2 Memory]]
https://www.adept.ai/blog/flashier-attention
https://tridao.me/blog/2024/flash3/
https://arxiv.org/abs/2208.07339
https://arxiv.org/abs/2307.08691