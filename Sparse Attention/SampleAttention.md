Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention
https://arxiv.org/pdf/2406.15486

Cites [[FlashAttention]] as being slower at Time to First Token so this technique is probably only relevant for inference.

