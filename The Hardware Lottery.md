https://arxiv.org/pdf/2009.06489
Google Brain paper by Sara Hooker

Core argument: It is our tooling which has played a disproportionate role in deciding what ideas succeed (and which fail).

The paper cites really old but fascinating sources like Rosenblatts [[Principles of Neurodynamics]] and [[The Jacquard Machine]], [[Cramming More Components onto Integrated Circuits]], [[Backpropagation]], [[Learning Representations by back-propagating errors]], [[Learning Matrices and Their Applications]]
[[Neocognitron]]

The paper posits that the 30 year gap between theoretical landmark papers and empirical success was largely caused by the lack of compatible hardware. 
General purpose computers are ill suited for matrix multiplication operations.
The need for parallelism in neural architectures was pointed out early in works like [[Parallel Models of Associative Memory]] and [[Parallel Distributed Processing]]

The contrast between the 2012 paper on [[Building High-level Features]] and the 2013 paper [[Deep Learnign with COTS HPC systems]], the former using 16000 CPUs, the latter only two CPU cores and four GPUs is striking. 

**Software lotteries**
Lisp and prolog were replaced by frameworks like Torch, Caffe, Theano, pytorch, etc.

### The Persistence of the Hardware lottery
