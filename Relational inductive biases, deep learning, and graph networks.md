https://arxiv.org/pdf/1806.01261

This paper is a treasure trove of classic papers in cognitive science.

A key signature of human intelligence is the ability to make "infinite use of finite means" [[On Language]], [[Aspects of the Theory of Syntax]], In which a small set of elements can be productively composed in limitless ways. 

This reflects the principle of combinatorial generalization: Constructing new inferences, predictions, and behaviors from known building blocks. 

In essence the paper lays out how AI research was forced to induce very strong inductive biases and structural assumptions about the data into their models because of lack of computing power. Modern methods have found great success on uninformative priors combined with vast amounts of data and compute. This paper argues that in joining these two models, with particular focus on graph based approaches, one may construct architectures greater than the sum of their parts. 

Section 2 of the paper explores the relational inductive biases present in several standard deep learning building blocks
